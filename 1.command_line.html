<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>RNA-seq command line analysis</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">RNA-seq Analysis Workshop</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">RNA-Seq workshop</a>
</li>
<li>
  <a href="1.command_line.html">Command line analysis</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">RNA-seq command line analysis</h1>

</div>


<head>
<script src="https://kit.fontawesome.com/ece750edd7.js" crossorigin="anonymous"></script>
</head>
<hr />
<div id="rna-seq-data" class="section level2">
<h2>RNA-seq data</h2>
<p>Intro on RNA sequencing, technologies, stranded/paired end etc…</p>
<hr />
</div>
<div id="getting-started" class="section level2">
<h2>1. Getting Started</h2>
<p>We are using the Linux command line to run most of the tools we use today. If you are new to Linux please complete the Intro to Command Line <a href="http://bifx-core.bio.ed.ac.uk/hyweldd/training/Bioinformatics_on_the_Command_Line/">Workshop</a>.</p>
<div id="logging-in" class="section level3">
<h3>Logging in</h3>
<p>We will use the WCB bioinformatics servers for this practical. If you do not have an account please contact a member of the core facility to get this set up. You will need access to the university network or VPN to access the server.</p>
<p>You will need to use a <strong>Terminal</strong> app on Mac, Linux or Windows or an alternative app such as <a href="https://mobaxterm.mobatek.net/">MobaXTerm</a>. You can also log in via <a href="https://wiki.x2go.org/doku.php/doc:installation:start">X2Go</a> if you would like to use a graphical interface.</p>
<p>To login via a command line application:</p>
<pre class="bash"><code>ssh USER@bifx-core2.bio.ed.ac.uk</code></pre>
<p>Once you have typed in your password, you should see some welcome text and a prompt that looks something like this:</p>
<pre class="bash"><code>[USERNAME]@bifx-core2:~$</code></pre>
<hr />
</div>
<div id="creating-a-web-directory" class="section level3">
<h3>Creating A Web Directory</h3>
<p>In order to view files created on the server, we need to create a <em>public_html</em> directory in our home folder.</p>
<p>After logging in you should be in your $HOME directory, check with;</p>
<pre class="bash"><code>pwd</code></pre>
<p>This should show the PATH of your present working directory, which should now be your home directory as you have just logged in. You can return to this place at any time using the change directory command.</p>
<pre class="bash"><code>cd</code></pre>
<p>You have permissions to create files and directories under your home folder. Lets create some now which we will use later on.</p>
<pre class="bash"><code>mkdir ~/public_html
mkdir ~/public_html/TMP</code></pre>
<p>Here we have used the absolute path name for each directory using ~/ as a shortcut for your $HOME directory. Nested directories are separated by the forward slash ‘/’ sign.</p>
<p>As you have created <code>~/public_html</code>, contents of this directory are available online with any web browser</p>
<p>To see it enter the following URL, changing <em>yourUserName</em> to whatever your username is: <a href="https://bifx-core2.bio.ed.ac.uk/~yourUserName" class="uri">https://bifx-core2.bio.ed.ac.uk/~yourUserName</a></p>
<hr />
</div>
<div id="integrative-genomics-viewer" class="section level3">
<h3>Integrative Genomics Viewer</h3>
<p>Please install <a href="https://software.broadinstitute.org/software/igv/download">IGV</a> on your own machine, alternatively you can use the <a href="https://igv.org/app/">web application</a>.</p>
<div class="key-points">
<h2>
<i class="fas fa-thumbtack"></i>Key Aims:
</h2>
<div id="section" class="section level2">
<h2></h2>
<ul>
<li>Make sure you can log in to the bioinformatics server</li>
<li>Create a personal web directory</li>
<li>Create a project directory for fastq files</li>
<li>Install IGV on your own machine</li>
</ul>
</div>
</div>
<p><br></p>
<hr />
</div>
</div>
<div id="rna-seq-sequencing-data" class="section level2">
<h2>2. RNA-seq sequencing data</h2>
<p>The datasets used in this exercise are derived from a single end ChIP-seq experiment (actually ChIP-exo) in <em>S.cerevisiae</em>. There are 2 biological replicates (though we recommend using 3 or more!) Reb1_R1 and Reb1_R2 as well as their corresponding input controls Input_R1 and Input_R2. For this experiment immunoprecipitation was performed with antibodies against Reb1. Reb1 recognizes a specific sequence (TTACCCG) and is involved in many aspects of transcriptional regulation by all three yeast RNA polymerases and promotes formation of nucleosome-free regions (NFRs). You can find the original publication <a href="http://www.sciencedirect.com/science/article/pii/S0092867411013511">here</a>. For the purpose of this workshop we have randomly subsampled and filtered out poor quality reads to speed up runtime.</p>
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Reb1_R1</td>
<td>ChIP experiment, replicate 1</td>
</tr>
<tr class="even">
<td>Reb1_R2</td>
<td>ChIP experiment, replicate 2</td>
</tr>
<tr class="odd">
<td>Input_R1</td>
<td>Input DNA, replicate 1</td>
</tr>
<tr class="even">
<td>Input_R2</td>
<td>Input DNA, replicate 2</td>
</tr>
</tbody>
</table>
<hr />
<div id="obtaining-data" class="section level3">
<h3>Obtaining data</h3>
<p>First, make a new directory for this tutorial and move into that directory. Then link the directory to your <code>public_html</code> folder as we are going to make everything public in this tutorial.</p>
<pre class="bash"><code>cd 
mkdir RNA-seq_workshop
cd RNA-seq_workshop
ln -s $PWD ~/public_html/</code></pre>
<p>Next, create a subfolder called <strong>fastq</strong> for all of our sequence files and link the raw datasets to this folder:</p>
<pre class="bash"><code>mkdir fastq
cp /homes/library/training/ChIP-seq_workshop/data/*fq.gz fastq/.</code></pre>
<p>When you receive data from a sequencing centre the file should also be provided with an alphanumeric string known as an <strong>md5 checksum</strong>. We can think of this as a files passport or fingerprint and use it to verify our data and ensure it wasn’t corrupted or truncated during download. The md5 checksums for these files are below. Lets check that now using the <code>md5sum</code> command:</p>
<table>
<thead>
<tr class="header">
<th>md5 checksum</th>
<th>filename</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>914b4dda687a76b0d50e545e3ce705d6</td>
<td>Input_R1.fq.gz</td>
</tr>
<tr class="even">
<td>f421ed18b71a801b236612cdde49dbaf</td>
<td>Input_R2.fq.gz</td>
</tr>
<tr class="odd">
<td>dd363301ad237ecb6c81c59ae97995a2</td>
<td>Reb1_R1.fq.gz</td>
</tr>
<tr class="even">
<td>06623f9e556876dd6c4d1dfdc4348698</td>
<td>Reb1_R2.fq.gz</td>
</tr>
</tbody>
</table>
<pre class="bash"><code>cd fastq #Move into the fastq directory
md5sum *.fq.gz &gt; md5
cat md5 #prints out the contents of md5
#To check the files and md5 sums match at any time
md5sum -c md5 </code></pre>
<hr />
</div>
<div id="fastq-files" class="section level3">
<h3>FastQ files</h3>
<p>Sequencing data will typically be provided to you in <strong>fastq</strong> format (.fq or .fastq) or as a compressed gzipped fastq (.fq.gz) in order to save space. We can view a gzipped file with the zless command, let’s take a look:</p>
<pre class="bash"><code>cd RNA-seq_workshop
cd fastq # Move into the fastq directory (if not already)
zless Control_1.fq.gz | head -n 12</code></pre>
<p><a href="https://en.wikipedia.org/wiki/FASTQ_format">Fastq</a> files contain 4 lines per sequenced read:</p>
<ul>
<li>Line 1 begins with an ‘@’ character and is followed by a sequence identifier and an optional description</li>
<li>Line 2 is the raw sequence</li>
<li>Line 3 begins with a ‘+’ character and is optionally followed by the same sequence identifier</li>
<li>Line 4 encodes the Phred quality score for the sequence in Line 2 as ASCII characters</li>
</ul>
<hr />
</div>
</div>
<div id="quality-control" class="section level2">
<h2>3. Quality control</h2>
<p>Next we want to assess the quality of our sequencing data and check for any biases and contamination.</p>
<hr />
<div id="fastq-screen" class="section level3">
<h3>FastQ screen</h3>
<p>When running a sequencing pipeline it is useful to know that your sequencing library contains the types of sequence they’re supposed to. <a href="http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/">FastQ Screen</a> allows you to set up a standard set of libraries against which all of your sequences can be searched. Your search libraries might contain the genomes of all of the organisms you work on, along with PhiX, Vectors or other contaminants commonly seen in sequencing experiments. We will run a screen of our sequences against a few default genomes:</p>
<pre class="bash"><code>cd .. #Move up a directory again
fastq_screen --conf /homes/genomes/tool_configs/fastq_screen/fastq_screen.conf fastq/*fq.gz --outdir fastq
# * is a wild card character</code></pre>
<p>Once complete take a look at the output images in your browser via your public_html folder. This shows that most of your reads align to the human genome and that no reads align uniquely to other organisms: <!--Update pic here?--> <img src="images/Reb1_R1_screen.png" width=1200></p>
<hr />
</div>
<div id="fastqc" class="section level3">
<h3>FastQC</h3>
<p><a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/">FastQC</a> provides simple quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before proceeding.</p>
<pre class="bash"><code>fastqc fastq/*.fq.gz</code></pre>
<p>FastQC will create report files for each of your datasets which we can view in the browser. We will go through each of the images during the workshop. For future reference, specific guidance on how to interpret the output of each module is provided in the <a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/">fastqc help pages</a>.</p>
<p>An example of poor quality sequencing at the end of short reads:</p>
<p><img src="images/Poor_sequence_quality.png" width=700></p>
<p>The software gives a <em>pass</em>, <em>fail</em> or <em>warning</em> flag for each test based on what we would expect from a regular DNA-sequencing run. It is important to realise that FastQC does not understand the origin of your data and that different datasets will have different characteristics. For instance RNA sequencing often involves the use of <strong>random hexamer primers</strong> that are <a href="https://sequencing.qcfail.com/articles/positional-sequence-bias-in-random-primed-libraries/">not as random as you might expect</a>. The profile below in the first ~15 bases is perfectly normal for these samples but will be flagged as an error by FastQC:</p>
<p><img src="images/Random_hexamer_priming.png" width=700></p>
<p>Visit the <a href="https://sequencing.qcfail.com/author/simon/">QCFail</a> website for more examples and advice on quality control for NGS datasets.</p>
<hr />
</div>
<div id="multiqc" class="section level3">
<h3>MultiQC</h3>
<p>We can view summaries of multiple reports at once by using <a href="http://multiqc.info/">multiqc</a>:</p>
<pre class="bash"><code>multiqc -o fastq fastq</code></pre>
<p>MultiQC searches for report files in a directory and compiles them into a single report. Open the multiqc report via a web browser to see how the raw datasets compare. Here we have the output of FastQ_screen and FastQC, but MultiQC works with the outputs of many tools other tools which we’ll see later.</p>
<p>If we look at the <em>adapter content</em> and <em>over represented sequences</em> sections we can see a small amount of contamination particularly at the ends of the sequencing reads.</p>
<hr />
</div>
</div>
<div id="shell-scripts-pipelining-and-parallelisation" class="section level2">
<h2>4. Shell scripts, pipelining and parallelisation</h2>
<p>Up until now we have run command line tools on each one of our datasets in serial, this means they run one after the other. In this tutorial we only have a few small datasets and the tools run relatively quickly, but this approach won’t scale well to multiple large datasets. A more efficient approach is to create a <strong>script</strong> that will run all of our datasets in <strong>parallel</strong>.</p>
<hr />
<div id="parallel" class="section level3">
<h3>parallel</h3>
<p>Unix has a program called <strong>parallel</strong> which allows you to run tools on multiple datasets at the same time. The following command would list all of your gzipped fastq files and pipe “|” them into parallel.</p>
<pre class="bash"><code>ls fastq/*fq.gz | parallel -j 5 fastqc {} &amp;
ps f</code></pre>
<ul>
<li><em>ls</em> lists files ending with .fq.gz in your fastq directory and pipes the names into the parallel command.</li>
<li>The parallel <em>-j</em> flag stands for <em>juggle</em> and tells parallel to run 5 processes at the same time.</li>
<li>In this case we are running fastqc and the <em>{}</em> is a place holder for the filenames we are piping in.</li>
<li>The <em>&amp;</em> character runs these jobs in the background so we can continue to use the terminal.</li>
<li><em>ps</em> is the <em>process status</em> tool which shows jobs running in the current session, we should see 5 instances of fastqc running.</li>
</ul>
<hr />
</div>
<div id="shell-scripts" class="section level3">
<h3>Shell scripts</h3>
<p>Putting all of your commands into a script is good practice for keeping track of your analysis and for reproducibility. We want to write scripts so they can be used again on different datasets and avoid <em>hardcoding</em> the names of files.</p>
<p>First, let’s create a file that lists our sample names so we can feed this into our pipeline. We could just type this manually or use a bit of text processing like the code below, here fastq files are ‘piped’ into parallel as above but we use regular expression within ‘sed’ to remove the name ending, this can now be used to name all files.</p>
<pre class="bash"><code>ls fastq/*fq.gz | parallel basename | sed s/.fq.gz// &gt; samples.txt</code></pre>
<p>Now let’s create our pipeline script. We’ll start by creating and opening a new file called <strong>pipeline.sh</strong>. The .sh stands for shell which is essentially just a name for the command line environment.</p>
<pre class="bash"><code>emacs -nw pipeline.sh</code></pre>
<p><strong>Emacs</strong> is a Unix text editor and the -nw flag opens a new window for editing. Here we are going to paste all of the commands we have used so far (click the mouse wheel to paste in the terminal):</p>
<pre><code>## PREPARATION

## Create a web directory to view output                         
ln -s $PWD ~/public_html/
## Create a folder for fastq files
mkdir fastq
## Link the raw data to the fastq folder
cp /homes/library/training/RNA-seq_workshop/fastq/*fq.gz fastq/
## Link the annotation folder
ln -s /homes/library/training/RNA-seq_workshop/annotation .

## QC

## FastQ Screen
fastq_screen --conf /homes/genomes/tool_configs/fastq_screen/fastq_screen.conf fastq/*fq.gz --outdir fastq

## FastQC
fastqc fastq/*.fq.gz

##Multi QC
multiqc -o fastq fastq

## PREPROCESSING </code></pre>
<p>The <strong>#</strong> symbol indicates a comment line and anything preceded by a # will not be run by the command line. It is good practice to comment your code.</p>
<hr />
</div>
<div id="challenge" class="section level3">
<h3>Challenge</h3>
<div class="challenge">
<h2>
<i class="fas fa-pencil-alt"></i> Challenge:
</h2>
<p>See if you can adapt the QC portion of the script to use parallel.</p>
<strong>Hint:</strong> <code>cat samples.txt</code> will print the names of all of the samples.
<details>
<summary>
Solution
</summary>
<div class="solution">
<h2>
<i class="far fa-eye"></i>
</h2>
<p><code>cat samples.txt | parallel -j 5 "fastq_screen --conf /homes/genomes/tool_configs/fastq_screen/fastq_screen.conf fastq/{}.fq.gz  --outdir fastq"</code></p>
<p><code>cat samples.txt | parallel -j 5 "fastqc fastq/{}.fq.gz"</code></p>
</div>
</details>
</div>
<p><br></p>
<p>To save and exit the file press <strong>ctrl-x</strong>, followed by <strong>ctrl-c</strong> and then <strong>y</strong> to save.</p>
<p>Let’s move on with our RNA-seq analysis. Every time we run a tool that creates new output we should add it to our pipeline.sh file. We will learn how to run this script and the full analysis in one go later on.</p>
<p>Although shell scripts save all of your commands they do not necessarily track the versions of software that you use or the current state of your environment and other tool dependencies. They are also unaware of the workflow of your data. More advanced methods of <em>pipelining</em> and <em>containerisation</em> are recommended for fully reproducible analysis and sustainable programming and we will see an example of this using <strong>NextFlow</strong> in the next session.</p>
<hr />
</div>
</div>
<div id="pre-processing-quality-trimming-and-adapter-removal" class="section level2">
<h2>5. Pre-processing: Quality trimming and adapter removal</h2>
<p>From the FastQC report we can see that the overall quality of our sequencing is good, however it is good practice to perform some pre-processing and filtering of reads. Poor quality sequencing can make a read less alignable, so it is good practice to <strong>quality trim</strong> the ends of reads until we get to the high quality portion. Trimming is not always necessary as some mapping programs will trim the reads for you or perform <strong>soft clipping</strong> where only part of a read is required to align, but studies have shown that pre-processing generally improves alignment rate if done correctly.</p>
<p>Sequencing libraries are normally constructed by ligating adapters to fragments of DNA or RNA. If your read length is longer than your fragment then <a href="https://sequencing.qcfail.com/articles/read-through-adapters-can-appear-at-the-ends-of-sequencing-reads/">sequenced reads will contain the adapter sequence</a>. <strong>Adapter removal</strong> is a necessary consideration for your QC workflow, especially if adapters are detected by FastQC.</p>
<p>An example of adapter contamination at the end of reads: <img src="images/Adapter_contamination.png" width=700></p>
<p>Once reads have been trimmed they will vary in length. You may want to <strong>filter</strong> out reads that are now too short to be uniquely mapped. Normally a cutoff of 20-30bp is standard.</p>
<hr />
<div id="trim-galore" class="section level3">
<h3>Trim Galore!</h3>
<p>In this workshop we will use <a href="https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/">Trim Galore!</a> for adapter and quality trimming. It is a wrapper around the popular tool <a href="http://cutadapt.readthedocs.org/en/stable/index.html">Cutadapt</a> which finds and removes unwanted sequences from high-throughput sequencing reads.</p>
<p>Cutadapt can perform quality trimming, adapter removal and read filtering as well as many other operations to prepare your reads for optimal alignment. We will run <strong>trim galore!</strong> with the following parameters:</p>
<ul>
<li>-q : Trim reads from the 3’ end with the given quality threshold (Phred score)</li>
<li>–length : Filter out reads below this length</li>
<li>–fastqc : Run fastqc on the trimmed reads</li>
<li>–illumina : Trim the standard Illumina adapter sequences</li>
<li>–cores : Use multiple CPU cores to speed up the computation</li>
<li>-o : the name of the output folder</li>
</ul>
<div class="blue">
<pre class="bash"><code>mkdir trim_galore ## Create a new folder for trimmed data
cat samples.txt | parallel -j 5 &quot;trim_galore --fastqc -q 20 --illumina --length 20 --cores 4 -o trim_galore fastq/{}.fq.gz&quot;</code></pre>
</div>
<p>To view a trim_galore report:</p>
<pre class="bash"><code>less trim_galore/Control_1.fq.gz_trimming_report.txt</code></pre>
<p>Let’s compare the fastqc reports. Run <strong>multiqc</strong> on the trimmed data and compare this with the reports for the raw data.</p>
<div class="blue">
<pre class="bash"><code>multiqc -f -o trim_galore trim_galore</code></pre>
</div>
<p>Remember to add the trim_galore and multiqc steps to your <strong>pipeline.sh</strong> file.</p>
<hr />
</div>
<div id="other-trimming-software-worth-investigating" class="section level3">
<h3>Other trimming software worth investigating</h3>
<p>Trim Galore simplifies the Cutadapt command and also runs fastqc on the trimmed data. However, it is worth looking through the full functionality of Cutadapt which has many options for filtering and trimming reads. <strong>Trimmomatic</strong> is another tool which has a very sensitive algorithm for detecting adapters, especially in paired-end reads.</p>
<ul>
<li><a href="https://cutadapt.readthedocs.io/en/stable/">Cutadapt</a></li>
<li><a href="http://www.usadellab.org/cms/?page=trimmomatic">Trimmomatic</a></li>
</ul>
</div>
<div id="further-qc-steps" class="section level3">
<h3>Further QC steps</h3>
<p>It may be worth looking into a few other tools for quality control of RNA-seq data.</p>
<ul>
<li><a href="https://bioinfo.lifl.fr/RNA/sortmerna/">SortMeRNA</a> can remove sequences that map to ribosomal RNAs. Ideally, your library preparation should include steps to deplete ribosomal RNAs but you can also use tools to remove these from your analysis.</li>
<li><a href="http://rseqc.sourceforge.net/">RSeQC</a> is a QC tool specifically for RNA-seq.</li>
</ul>
<div class="key-points">
<h2>
<i class="fas fa-thumbtack"></i>Key Aims:
</h2>
<div id="section-1" class="section level2">
<h2></h2>
<ul>
<li>Check for contaminants</li>
<li>Assess sequence quality</li>
<li>Understand scripting</li>
<li>Trim and filter your sequencing reads</li>
</ul>
</div>
</div>
<hr />
</div>
</div>
<div id="read-alignment" class="section level2">
<h2>6. Read Alignment</h2>
<p>There are many tools available for mapping reads to genomes and transcriptomes, each with their own purposes and strengths. RNA-seq data requires a <em>splice-aware</em> aligner that can handle splice junctions in the sequencing reads. The two most popular aligners are <a href="https://github.com/alexdobin/STAR">STAR</a>, which is very fast, and <a href="http://daehwankimlab.github.io/hisat2/manual/">HISAT2</a>, which is very accurate.</p>
<div class="figure">
<img src="http://bifx-core.bio.ed.ac.uk/training/NGS_workshop/images/splice_mapping.png" alt="" />
<p class="caption">spliced</p>
</div>
<div id="genome-assemblies-and-indexing" class="section level3">
<h3>Genome assemblies and indexing</h3>
<p>First, we need to select a reference genome to align to. Every time a reference genome is released or updated it is given a new name, often referred to as the genome build or assembly (..hg18, hg19, hg38). It is important to realise that different builds of the same genome are different sequences and thus their co-ordinate systems are incomparable. For instance position 10000000 on chr1 is T in hg19 and G in hg38.</p>
<p>We are going to map our reads to the latest release of the human genome (<strong>hg38</strong> or <strong>GRCh38</strong>). We need to create an index file from the GRCh38 fasta sequence so that STAR can quickly access the reference sequences. Many of these indexes are pre-computed on our servers and stored under the /homes/genomes directory for everyone to use. The indexes for this workshop have are available at **.</p>
<p>The code below shows how these were generated with STAR. You do not need to run this again.</p>
<pre class="bash"><code>## Download sequence files and latest annotations from Ensembl
#wget http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz  ## Use primary assembly without Alt contigs but including scaffolds
#wget http://ftp.ensembl.org/pub/release-106/gtf/homo_sapiens/Homo_sapiens.GRCh38.106.gtf.gz

#gzip -d Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz
#gzip -d Homo_sapiens.GRCh38.106.gtf.gz

## Build STAR index
#STAR --runMode genomeGenerate --genomeDir STAR_index_hg38.ensembl106 --genomeFastaFiles Homo_sapiens.GRCh38.dna.primary_assembly.fa --sjdbGTFfile Homo_sapiens.GRCh38.106.gtf --sjdbOverhang 100 --runThreadN 20</code></pre>
</div>
<div id="mapping-reads-with-star" class="section level3">
<h3>Mapping reads with STAR</h3>
<p>Now that we have an index we can align our reads to the human genome with STAR.</p>
<div class="blue">
<pre class="bash"><code>mkdir STAR ## make a new folder for read alignments
cat samples.txt | parallel -j 5 &quot;STAR --genomeDir annotation/STAR_index_hg38.ensembl106 --readFilesCommand zcat --readFilesIn sub/trim_galore/{}_trimmed.fq.gz --runThreadN 4 --outSAMtype BAM SortedByCoordinate --outFileNamePrefix sub/STAR/{}. --sjdbGTFfile annotation/Homo_sapiens.GRCh38.106.gtf --outWigType wiggle --outWigNorm RPM&quot;</code></pre>
</div>
<p>We are using the following parameters:</p>
<ul>
<li>–genomeDir : The location of the STAR genome index</li>
<li>–readFilesInCommand : The unix command for reading a *.gz file</li>
<li>–readFilesIn : The location of the fq.gz file</li>
<li>–runThreadN : The number of CPU threads available to each STAR process</li>
<li>–outSAMtype : The output file format</li>
<li>–outFileNamePrefix : Used to name the outputs</li>
<li>–sjdbGTFfile : A gene model and splice junction annotation file in gtf format</li>
<li>–outWigType : Additional output of read coverage profile</li>
<li>–outWigNorm : Normalisation strategy for read coverage profile</li>
</ul>
<p>Note that we do not need to include the –sjdbGTFfile here as it was already used in creating the index. We can provide the annotations at either step or provide a different file with additional annotation.</p>
<p>We recommend reading the <a href="https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf">STAR manual</a> to fully understand all of the parameters and output options. Running alignment software with default parameters may not be the best option for your data.</p>
<p>When your alignment has completed, take a look at the output STAR report:</p>
<pre class="bash"><code>less STAR/Control_1.Log.final.out</code></pre>
<ul>
<li>How many aligned reads do you have?</li>
<li>How many of these align to a single location on the genome?</li>
<li>How many reads map to multiple locations?</li>
</ul>
</div>
<div id="sambamcram-format-and-samtools" class="section level3">
<h3>SAM/BAM/CRAM format and Samtools</h3>
<p>The standard output for most mapping software is SAM (sequence alignment/map format). SAM files contain many columns that describe the position of each alignment as well as information on the quality of the alignment, mismatches, the number of times a read mapped, mapping of paired ends and other custom flags and statistics. SAM files can be very large so there are compressed alternatives BAM and CRAM. The <a href="http://www.htslib.org/doc/samtools.html">samtools</a> package has many useful tools for viewing, filtering and manipulating files in SAM format. We will use some of these below.</p>
<p>Take a look at the <a href="https://samtools.github.io/hts-specs/SAMv1.pdf">SAM format specification</a> and the first few lines of your BAM output using <code>samtools</code>.</p>
<pre class="bash"><code>samtools view STAR/Control_1.Aligned.sortedByCoord.out.bam | less</code></pre>
<p>The second column is the <strong>SAM flag</strong> and contains coded information about each alignment. Use the <a href="https://broadinstitute.github.io/picard/explain-flags.html">Explain SAM flags</a> resource to find out more about the alignments in your file.</p>
<p>We can also see the samtools header using the -h flag which contains information on the parameters and indexes used to create the file.</p>
<pre class="bash"><code>samtools view -h STAR/Control_1.Aligned.sortedByCoord.out.bam | less</code></pre>
<p>We can index our bam file for quick programmatic access to the binary file. The file <em>STAR/Control_1.Aligned.sortedByCoord.out.bam.bai</em> is created and is essential for some of the samtools programs to work (also for genome browsers).</p>
<div class="blue">
<pre class="bash"><code>cat samples.txt | parallel -j 5 &quot;samtools index STAR/{}.Aligned.sortedByCoord.out.bam&quot; ## Index the bam files</code></pre>
</div>
<p>The command <code>samtools idxstats</code> outputs the number of reads aligned to each sequence in our reference.</p>
<pre class="bash"><code>samtools idxstats STAR/Control_1.Aligned.sortedByCoord.out.bam</code></pre>
<div class="discussion">
<h2>
<i class="far fa-bell"></i> Discussion
</h2>
<p>The third column represents the number of alignments reads. What can you assume from the alignments to the Y chromosome?</p>
</div>
<p><br></p>
<hr />
</div>
</div>
<div id="post-alignment-processing" class="section level2">
<h2>7. Post alignment processing</h2>
<div id="filtering-reads-with-samtools" class="section level3">
<h3>Filtering reads with samtools</h3>
<p>Now that we have aligned our reads we may want to do some filtering before any downstream analysis. Make sure you are aware of the alignments that are reported by your mapping program and the parameters used. For instance, are unmapped reads included in the BAM file? Are all alignments to repeats reported or just one? Are paired-end alignments still reported if only one end maps?</p>
<p>There are many ways to filter your BAM files with samtools and other programs to remove unwanted alignments that may negatively affect your downstream analysis. First lets look at the alignments contained in one of our bam files with <code>samtools flagstat</code>:</p>
<pre class="bash"><code>samtools flagstat STAR/Control_1.Aligned.sortedByCoord.out.bam</code></pre>
<p>Flagstat seems to report more alignments than the number of reads shown in our STAR report, how can this be? Our BAM file contains 1 record per alignment and some reads may align to multiple locations. We can use samtools view -f to include, and -F to exclude reads with a given SAM flag.</p>
<p>The flag 260 is the sum of 4 (read is unmapped) and 256 (not primary alignment) to give us only primary alignments. STAR assigns one of the top scoring alignments per read as primary. The <code>-b</code> option tells samtools view to output in BAM format.</p>
<div class="blue">
<pre class="bash"><code>cat samples.txt | parallel -j 5 &quot;samtools view -F 260 -bh -o  STAR/{}.primary.bam STAR/{}.Aligned.sortedByCoord.out.bam; samtools index STAR/{}.primary.bam&quot;</code></pre>
</div>
<p>If we run <code>samtools flagstat</code> on this filtered bam file we should now have one alignment per mapped read.</p>
<pre class="bash"><code>samtools flagstat STAR/Control_1.primary.bam</code></pre>
</div>
<div id="multimap-reads-and-duplicate-reads" class="section level3">
<h3>Multimap reads and Duplicate reads</h3>
<p>Multimap and duplicate reads are often confused so it is important to understand what these are and how they affect your data:</p>
<ul>
<li><strong>Multimap reads</strong> = The read exists once in your library and aligns to multiple repeat locations in the reference genome.</li>
<li><strong>Duplicate reads</strong> = Multiple unique reads with the same sequence that align to identical locations in the genome.</li>
</ul>
<p><img src="images/multi.png" alt="pic" width="600"/></p>
<p><strong>Multimap reads</strong> are difficult to analyse as their ambiguity can confound results. Many applications require the use of <strong>unique</strong> alignments only, thus multimap reads need to be removed from your BAM file. Aligners assign a <strong>mapping quality</strong> to each read (column 5 in BAM) between 0 and 255 that describes its confidence in the alignment position. Assigned <a href="https://sequencing.qcfail.com/articles/mapq-values-are-really-useful-but-their-implementation-is-a-mess/">mapping qualities differ between mappers</a>. STAR uses mapping quality scores of 0 and 255 to represent multimap and unique reads respectively. Filtering out reads with a mapping quality &lt; 255 means that all remaining reads align to a single unique position. We can use <code>samtools view -q</code> to filter based on mapping quality.</p>
<div class="blue">
<pre class="bash"><code>cat samples.txt | parallel -j 4 &quot;samtools view -b -q 20 STAR/{}.primary.bam -o STAR/{}.unique.bam; samtools index STAR/{}.unique.bam&quot;</code></pre>
</div>
<p><strong>Duplicate reads</strong> are often observed as tall spikes in your read depth profile where reads are stacked directly on top of each other. A high level of duplication in your library may be due to over amplification by PCR or contamination. In RNA-seq experiments the chances of selecting identical sequences are much higher as our sample size is much smaller (length of transcriptome vs length of genome) and multiple copies of the same sequence (transcripts) exist independently. Thus, it is common practice to retain duplicate reads.</p>
<div class="resources">
<h2>
<i class="fas fa-book"></i> Further Learning
</h2>
<p>Learn more about duplication and removing duplicate reads:</p>
<ul>
<li><a href="https://sequencing.qcfail.com/articles/libraries-can-contain-technical-duplication/">Duplication bias</a></li>
<li>The <a href="https://broadinstitute.github.io/picard/index.html">Picard</a> package has many useful utilities for manipulating SAM/BAM files. The <strong>MarkDuplicates</strong> tool will check the alignment positions for duplicate reads and mark or remove them from your data depending on how you wish to treat them.</li>
</ul>
</div>
<p><br></p>
</div>
<div id="removing-alignments-to-regions-of-the-genome" class="section level3">
<h3>Removing alignments to regions of the genome</h3>
<p>In some cases you may wish to remove reads that align to specific regions of the genome. For instance, if you still have a lot of ribosomal RNAs that will skew your downstream analysis.</p>
<ul>
<li><a href="https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html">bedtools intersect</a> can remove reads from BAM files that align to specific genomic locations.</li>
</ul>
<div class="key-points">
<h2>
<i class="fas fa-thumbtack"></i>Key Aims:
</h2>
<div id="section-2" class="section level2">
<h2></h2>
<ul>
<li>Align reads to a reference genome</li>
<li>Understand key file formats</li>
<li>Filter alignments for further analyses</li>
</ul>
</div>
</div>
<p><br></p>
<hr />
</div>
</div>
<div id="visualising-alignments-on-a-genome-browser" class="section level2">
<h2>8. Visualising alignments on a genome browser</h2>
<p>Visually inspecting data via a genome browser is often the first step in any analysis. Has the sequencing worked as expected? Are there noticeable differences between my samples by eye? Do RNA-seq reads align to the expected strand?</p>
<div id="converting-bam-files-to-bigwig" class="section level3">
<h3>Converting BAM files to bigWig</h3>
<p>BAM files contain information about each read, which can be great if you want to look at individual alignments and splice junctions, but they are typically large and slow to work with on a genome browser. If we are only interested in read coverage we can convert our BAM files to graphs of sequencing depth per base.</p>
<p>The <strong>wiggle</strong> file format has three columns to represent genomic co-ordinates and a 4th as a score which will represent the number of reads overlapping that region. A compressed version of this format, called <a href="https://genome.ucsc.edu/goldenPath/help/bigWig.html"><strong>bigWig</strong></a>, exists for use with genome browsers.</p>
<p>You will have noticed that we already asked STAR to output wiggle files which are normalised to read depth (RPM = Reads per million bases). STAR gives us wiggle files for both unique and unique + multimap reads and also splits these up by strand. Let’s take a look at one of these files:</p>
<pre class="bash"><code>head STAR/Control_1.Signal.Unique.str1.out.wig</code></pre>
<p>Let’s convert the <strong>unique</strong> files to bigWig format for visualisation. The tool <strong>wigToBigWig</strong> requires three arguments, an input file, a file of chromosome lengths, and the name of the output file.</p>
<div class="blue">
<pre class="bash"><code>mkdir visualisation ## Create a folder for visualisation files
cat samples.txt | parallel -j 5 &quot;wigToBigWig STAR/{}.Signal.Unique.str1.out.wig annotation/Homo_sapiens.GRCh38.dna.primary_assembly.len visualisation/{}.unique.r.bw; wigToBigWig full/STAR/{}.Signal.Unique.str2.out.wig annotation/Homo_sapiens.GRCh38.dna.primary_assembly.len full/visualisation/{}.unique.f.bw&quot;</code></pre>
</div>
<div class="resources">
<h2>
<i class="fas fa-book"></i> Further Learning
</h2>
<p>If you would like to create <em>bigWig</em> files directly from BAM files, e.g. after filtering or from other aligners, we recommend using <a href="https://deeptools.readthedocs.io">deepTools bamCoverage</a>. <strong>bamCoverage</strong> has lots of options for filtering, smoothing and normalising read coverage profiles.</p>
<p>Some analysis pipelines use <a href="https://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html">genomeCoverageBed</a> but be aware that this may not output normalised values.</p>
</div>
<p><br></p>
</div>
<div id="viewing-data-on-igv" class="section level3">
<h3>Viewing data on IGV</h3>
<hr />
</div>
</div>
<div id="quantifying-expression-levels" class="section level2">
<h2>9. Quantifying expression levels</h2>
<p>Once we have inspected our data and are happy with our alignments, we can quantify expression levels for annotated transcripts. Pseudo-alignment tools, such as <a href="https://salmon.readthedocs.io/en/latest/salmon.html">Salmon</a>, map sequencing reads directly to transcripts. They are extremely fast, can handle multi-map reads, and take into account multiple isoforms, transcript lengths and sequencing biases when estimating abundance.</p>
<hr />
<div id="create-a-salmon-index" class="section level3">
<h3>Create a Salmon index</h3>
<p>Like STAR, Salmon requires an index for alignment. The indexes require fasta sequences of all coding and non-coding RNAs as well as the genomic sequence. The Salmon index for this workshop has been pre-computed using the code below. It will also be in the annotation folder you linked previously.</p>
<pre class="bash"><code>## Download cdna and ncrna fasta files from Ensembl
#wget http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz
#wget http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/ncrna/Homo_sapiens.GRCh38.ncrna.fa.gz

## Create Salmon indexes

## Concatenate hg38 cdna and ncrna
#zcat Homo_sapiens.GRCh38.cdna.all.fa.gz Homo_sapiens.GRCh38.ncrna.fa.gz &gt; Homo_sapiens.GRCh38.cdna.ncrna.fa.tmp

## Make sure header is transcript name only and remove transcript version number
#perl -lane &#39;if(m/^&gt;/){$id=(split &quot; &quot;,$_)[0];$id=(split &quot;\\.&quot;,$id)[0];print $id;}else{print $_;}&#39; Homo_sapiens.GRCh38.cdna.ncrna.fa.tmp  &gt; Homo_sapiens.GRCh38.cdna.ncrna.fa
#rm Homo_sapiens.GRCh38.cdna.ncrna.fa.tmp

## Get genome chr sequences in decoys.txt
#grep &quot;^&gt;&quot; Homo_sapiens.GRCh38.dna.primary_assembly.fa | sed s/&quot;&gt;&quot;//g | cut -f 1 -d &quot; &quot; &gt; decoys.txt

## Cat transcriptome and genome to make gentrome file: 
#cat Homo_sapiens.GRCh38.cdna.ncrna.fa Homo_sapiens.GRCh38.dna.primary_assembly.fa &gt; Homo_sapiens.GRCh38.cdna.ncrna.gentrome.fa
#gzip Homo_sapiens.GRCh38.cdna.ncrna.gentrome.fa

## Build Salmon index
#salmon index -t Homo_sapiens.GRCh38.cdna.ncrna.gentrome.fa.gz -d decoys.txt -p 24 -i hg38.cdna.ncrna.salmon.index -k 31</code></pre>
<div class="resources">
<h2>
<i class="fas fa-book"></i> Further Learning
</h2>
<p>Creating Salmon indexes is more complicated than STAR but the bioinformatics team can help you out if you need to do this. You can also read more about the method <a href="https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment">here</a>.</p>
</div>
<p><br></p>
</div>
<div id="pseudo-alignment-with-salmon" class="section level3">
<h3>Pseudo-alignment with Salmon</h3>
<p>We can now run Salmon to estimate expression levels of transcripts. We use the <code>salmon quant</code> command with the following arguments:</p>
<ul>
<li>-i : location of the index files</li>
<li>-r : location of the fq.gz file (use -1,-2 for paired end files)</li>
<li>-l : library type (read the docs to see what this means)</li>
<li>–seqBias : correct sequencing bias in the data</li>
<li>–gcBias : correct GC bias in the data</li>
<li>-o : name of the output folder</li>
</ul>
<div class="blue">
<pre class="bash"><code>mkdir salmon ## Make a folder for salmon output
cat samples.txt | parallel -j 5 &quot;salmon quant -i annotation/hg38.cdna.ncrna.salmon.index -l SR -r trim_galore/{}_trimmed.fq.gz -p 5 --seqBias --gcBias -o salmon/{}&quot;</code></pre>
<p>Biases in sequencing data can lead to over/under estimation of expression of certain transcripts. As you can see, Salmon has some functionality to overcome this.</p>
<p>The main output of Salmon is a <em>quant.sf</em> file, take a look at one of these files:</p>
<pre class="bash"><code>less salmon/Control_1/quant.sf</code></pre>
<p>For each transcript we have five columns:</p>
<ul>
<li>The transcript identifier</li>
<li>The transcript length (in bp)</li>
<li>The effective length (described in detail below)</li>
<li>TPM (transcripts per million), which is computed using the effective length</li>
<li>The estimated read count (‘pseudocount’)</li>
</ul>
<div class="discussion">
<h2>
<i class="far fa-bell"></i> Discussion
</h2>
<p>What exactly is the effective length? The sequence composition of a transcript affects how many reads are sampled from it. While two transcripts might be of identical actual length, depending on the sequence composition we are more likely to generate fragments from one versus the other. The transcript that has a higer likelihood of being sampled, will end up with the larger effective length. The effective length is the transcript length which has been “corrected” to include factors due to sequence-specific and GC biases.</p>
</div>
<p><br></p>
</div>
<div id="finish-your-script" class="section level3">
<h3>9. Finish your script</h3>
<p>We now have our alignments (BAM), visualisation files (bigWig) and transcript quantification (quant.sf) and this is normally a branching point for downstream analyses.</p>
<p>Your pipeline.sh script should look something like <a href="http://bifx-core.bio.ed.ac.uk/training/ChIP-seq_workshop/pipeline.sh">this</a> and we can run everything from start to end in one go. This could take….</p>
<pre class="bash"><code>mkdir RNA-seq_workshop_tmp #Create a temporary directory
cd  RNA-seq_workshop_tmp #Move into that directory
cp ../pipeline.sh ../samples.txt . #Copy the pipeline and samples file into the new directory
ln -s ../annotation . #Link the annotation folder to this new directory
bash pipeline.sh &gt; pipeline.log &amp; #Run the shell script (See Below)
cd .. #Move back to the main directory</code></pre>
<ul>
<li>We use the unix tool <strong>bash</strong> and the name of our script to run our commands through the shell.</li>
<li>We then redirect <strong>&gt;</strong> the output to a log file to keep track of any errors.</li>
<li>We use the <strong>&amp;</strong> to run everything in the background to continue using the terminal.</li>
</ul>
<p>You can keep track of your pipeline by using <strong>ps</strong> or looking at the log file.</p>
<pre class="bash"><code>ps f
tail RNA-seq_workshop_tmp/pipeline.log #Shows the end of a file</code></pre>
<hr />
</div>
<div id="tidy-up" class="section level3">
<h3>Tidy Up!</h3>
<p>Files are large and disk space is expensive, remove any unwanted or temporary files from your folder. We should always keep the raw data (fastq) and our final processed datasets (BAM, bigWig etc) and the script we used to generate them. Where you can, convert files to compressed or binary formats to save space e.g. fq to fq.gz, SAM to BAM, wig to bigWig.</p>
<div class="blue">
<pre class="bash"><code>rm trim_galore/*trim*fq.gz #Remove trimmed fastq temporary files
rm STAR/*/*.wig #Remove wig files once converted to bigWig</code></pre>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
